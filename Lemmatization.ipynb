{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flying-salad",
   "metadata": {},
   "source": [
    "## Lemmatization:\n",
    "\n",
    "If we checks on wikipedia, the definition for Lemmatization is 'the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form'. \n",
    "\n",
    "In NLP, this process aims to find the correct lemma for a word in a text. For instance, for the word \"running\", get the lemma \"run\", and not confuse with running as andjective (like in \"running shoes\"). \n",
    "\n",
    "There are many ways to get the root of a word (or stem), and it's depends on our objectives or tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "national-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import normalization\n",
    "from scripts.tokenizer import SpacyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "innocent-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"I love to listen to some music when I'm running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "typical-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = SpacyTokenizer('en_core_web_lg')\n",
    "#To run this must have downloaded the spacy model. You could dowloading via delivery/install_modules. Check Readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences using spacy and whitespace tokenization\n",
    "tokenized_whitespace = spacy_tokenizer.wsp_tokenizer(sent)\n",
    "tokenized_whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-charger",
   "metadata": {},
   "source": [
    "As result we got a list so tokens, just as they appears in the original sentence. But to get the lemmas for each one of them, we need some thing else. We can start trying to get stems for each token, as first approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stm = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming with SnowballStemmer just for instance\n",
    "stemmed_sent = [stm.stem(word) for word in tokenized_whitespace]\n",
    "stemmed_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-interface",
   "metadata": {},
   "source": [
    "This stems we got, appears be the correct ones, but it must be said, this process does not takes in count the morphological category for the analyzed token. One way to improve our results could rest in consider the POS tag of the token to get its correct lemma. For instance, we could think the word \"running\". It could correspond with the meanning of \"execution\" (NOUN) of some process or, on the other hand, could be a VERB: 'to run'. More over, In our example we can observe that \"'m\" token does not correspond with the lemma of the verb 'to be'. So, we can instantiate a lemmatizer, that takes the morphological information od the word in count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize sentences using spacy. This method keeps stopwords and return lemmas for every token \n",
    "lematized_sentence = spacy_tokenizer.lemmatizer(sent)\n",
    "lematized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-hearing",
   "metadata": {},
   "source": [
    "In this case, we finally can see how \"'m\" token was recognized as a inflexion of 'to be'. And we finally got a list of lemmas from the original sentence. \n",
    "\n",
    "\n",
    "\n",
    "But our sentence, or our list of lemmas has a lot of non-meanning words. I mean, words like 'to' or the pronoun 'I', for instance, does not gives us too much information for this tasks. If we want to compare two or more sentences, We could start cleanning the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method cleans stopwords and pronouns and returns only relevant lemmas from a sentence \n",
    "clean_base_sentence = spacy_tokenizer.lemma_tokenizer(sent)\n",
    "clean_base_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-waterproof",
   "metadata": {},
   "source": [
    "This last method could be usefull for as, if we would calculate similarity between two or more sentences. For instance, we could use it as input to get a TF-IDF score to measure their similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-heather",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "### TFIDF \n",
    "or Lexical Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marine-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.vectorizers import TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "packed-alpha",
   "metadata": {},
   "source": [
    "\"tf-idf is short term for term frequency-inverse document frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ruled-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TFIDFVectorizer(spacy_tokenizer.lemmatizer)\n",
    "#Can pass any tokenizer of class SpacyTokenizer or other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-commission",
   "metadata": {},
   "source": [
    "This tokenizer get the lemmas for each token and only keeps the meanning word of the sentence. Stopwords and pronouns are not included in the result to try avoid a harmfull score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mysterious-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = \"I love to listen to some music when I'm running\"\n",
    "\n",
    "sentences = [\n",
    "    \"I like to run and listen rock music\",\n",
    "    \"I love music\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proper-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most_similar_sentence': 'I love music', 'score': 0.6670239951717103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando/.local/share/virtualenvs/similarity-v-csMjBy/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.calculate_tfidf(sent, sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-frequency",
   "metadata": {},
   "source": [
    "If our aim is get a real similarity measurement we need to use better approaches. \n",
    "Anyway, TF-IDF Is good to set a baseline, for instance.  Until now, if two sentences share some words, specific words or parts of them, we could encounter with non happy results. We didnt consider False positive cases until now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "engaging-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_sent = \"I really want to share it with you\"\n",
    "fp_sent2 = [\n",
    "    \"I don't want to share it with you\",\n",
    "    \"I want to share it with you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "speaking-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most_similar_sentence': \"I don't want to share it with you\", 'score': 0.6410554491745126}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.calculate_tfidf(fp_sent, fp_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-panama",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "But we could use word embeddings to improve our manning of 'similarity'. Using specific domain embeddings trained with specific domain data, our similarity measurement could change. This kind of semantic similarity could be usefull for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.vectorizers import FastTextVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-disaster",
   "metadata": {},
   "source": [
    "As I have no specification about the domain or data specific to train a domain specific word ebeddings, I choosed one of the fasttext lib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "corresponding-effects",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastTextVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c35ce8765b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfasttext_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastTextVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/fasttext/cc.en.300.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# I will use these random fasttext model to evaluate similarity between one sentence and other(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loading could be slow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Could accept a custom trained vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FastTextVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "fasttext_vectorizer = FastTextVectorizer('models/fasttext/cc.en.300.bin')\n",
    "\n",
    "# I will use these random fasttext model to evaluate similarity between one sentence and other(s). \n",
    "# Loading could be slow.\n",
    "# Could accept a custom trained vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import cosine_similarity as cs\n",
    "\n",
    "def eval_similarity(sents_list, vectorizer):\n",
    "    vectorized_sents = vectorizer.vectorize_sentences(sents_list)\n",
    "    similarity = cs.cosine_one_many(vectorized_sents[0], vectorized_sents[1:])\n",
    "    max_sim = max(similarity)\n",
    "    max_sim_idx = similarity.index(max_sim)+1 #Revisar implementaciòn de cosine_one_many\n",
    "    sent_idx_sim = sents_list[max_sim_idx]\n",
    "    \n",
    "    result = dict(most_similar_sentence=sent_idx_sim, score=max_sim)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences  = [sent] + sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_similarity(all_sentences, fasttext_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-stevens",
   "metadata": {},
   "source": [
    "## Disclaimers:\n",
    "To use these fasttext embeddings maybe was not the best way to achive this task, at least in terms of my owns machine resourses (model loading takes to much time, and invest it for compare two sentences, i guess was not the best experice)\n",
    "\n",
    "It must be improve, but as first direction I would look is the training of an specific word embeddings using fassttext or trying to use a model of BERT with a custom trained layer, but it all depends on the task and the type of data that I had. \n",
    "\n",
    "Another way to improve the similarity measurement, maybe could be parsing the sentences to attend to the dependencies of the tokens inside a sentence. I must be honest, i've never did it before, but I know of some pappers that attends to this way. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-termination",
   "metadata": {},
   "source": [
    "# 3. Problem: \n",
    "#### Suppose you had a corpus of corporate agreements, for example: vendor agreements, NDAs, privacy policies, etc. Describe your thoughts on building a model that is able to cluster the documents so that you would expect NDAs to be in one cluster and privacy policies in another cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-graphic",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Depending on the quantity of the data, or if I had this data labeled or not, for instance to decided if I go for a supervised or unsupervised process. But as a first approach may be I would try to implement a SVM as I did it before, for instance. \n",
    "We could build a trainingset for each class on our classifier and try to get better results as improve the training process. \n",
    "\n",
    "\n",
    "On the other hand, if I will choose to go by the unsupervised path, may be I could try to implement some clustering algorithms (as KMeans or another), suppossing K=2, but the contra here could be determine this number or if this is the correct according our real domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-castle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-hearing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-creation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-v-csMjBy",
   "language": "python",
   "name": "similarity-v-csmjby"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
